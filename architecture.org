* Distributed basics
checksum: usually used to guard accidental modification.
- weak but fast and easy.

* TODO twitter design
** notify model
*** push model
* GFS
** Same structure of a basic file system
- MetaInfo
  - Name
  - CreatedTime
  - Size
  - Indice
    - chunk0 -> disk offset
    - chunk1 -> disk offset
    - ...
- Chunks Storage
  - chunk0
  - chunk1
** Distributed File System for Big Data
- bigger chunck size 64MB
  - advantages:
    - smaller meta data, better for better network transmission
  - disadvantages
    - may waste storeage with small files
- one chunk with multiple blocks
  - one block, 64k
  - each block has a 32-bit checksum, to check file is broken
- three replica for data recovery
  - if a ChunckServer's data is broken, request Master to find other replicas
  - choose the server with low disk usage to store replica.
  - store two in a data center, the third one in another data center
  - one primary, two replica
- heat beat to detect ChunckServer's failure
** Reading
1. Client request Master and get the address of primary ChunkServer
2. request primary ChunkServer, and get the chunk
** Writing
1. Client request Master
2. Master send the addresses of primary and two replicas
3. write to the nearest server first, sequentially copy between servers
4. write to memory first to reduce disk operation frequency.
5. when all the ChunckServers ready, send the instruction to write to disk.
6. if any stage failed, the client just repeat.
** Reference
https://www.bittiger.io/classpage/QPQAy2DFkqLwHBS4K
* wechat architecture
- how to send receive messages
* TODO Paxos
* TODO Bigtable
* TODO Chubby
* TODO KV-store
** basics
** parameter server
** data disaster recovery
* TODO data compression
* CS latency numbers everyone should know
L1 cache reference                           0.5 ns
Branch mispredict                            5   ns
L2 cache reference                           7   ns                      14x L1 cache
Mutex lock/unlock                           25   ns
Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache
Compress 1K bytes with Zippy             3,000   ns        3 us
Send 1K bytes over 1 Gbps network       10,000   ns       10 us
Read 4K randomly from SSD*             150,000   ns      150 us          ~1GB/sec SSD
Read 1 MB sequentially from memory     250,000   ns      250 us
Round trip within same datacenter      500,000   ns      500 us
Read 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory
Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip
Read 1 MB sequentially from disk    20,000,000   ns   20,000 us   20 ms  80x memory, 20X SSD
Send packet CA->Netherlands->CA    150,000,000   ns  150,000 us  150 ms

[live demo](https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html)

** Notes
1 ns = 10^-9 seconds
1 us = 10^-6 seconds = 1,000 ns
1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns
* save bits to store data
* some basic tools
** memcached (as a cache)
*** basic API
- set, add or overwrite this key
- add, store this data only if it is exists.
- cas (Check and Set), resolve race condition.
- get
- gets, use cas
*** Simple key/value store
the value should be pre-serialized.
*** Logic half in client, half in server
- client knows how to choose which server to read or write.
- servers knows how to store and fetch items.
*** Servers are disconnected from each other
- there are no synchronization
*** Forgetting is a feature 
- LRU
- items expire after a specified amount of time
  - avoid stale data being returned
** mangoDB(as a NoSQL)
** redis(as a NoSQL)
** rabbitMQ message queue
